{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout\n",
    "# Using Deep Q-Learning method. \n",
    "http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html\n",
    "\n",
    "DQN paper:\n",
    "\n",
    "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "\n",
    "https://pdfs.semanticscholar.org/presentation/a210/9d4f149e1526907fc0c907458656bca6f73e.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience replay (off policy, random sample)\n",
    "\n",
    "breack the relation of updating weight. (two methods)\n",
    "\n",
    "1:Experience replay (off policy, random sample)\n",
    "\n",
    "2:Fixed Q-targets (two network !!! new $Q_{value}$ and old $Q_{target}$ )\n",
    "\n",
    "   old $Q_{target}$ is using for update network. $R+r*maxQ'$\n",
    "   \n",
    "   new Q is $Q_{value}$.\n",
    "   \n",
    "   $NN = nn + \\alpha * ( Q_{target} - Q_{value} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadding library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanhua/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import prettytensor as pt\n",
    "import scipy.ndimage\n",
    "import time\n",
    "import sys\n",
    "import reinforcement_learning as rl\n",
    "from reinforcement_learning import ReplayMemory\n",
    "\n",
    "RENDER = False  # rendering wastes time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "\n",
    "MEMORY_SIZE = 3000\n",
    "ACTION_SPACE = 11\n",
    "\n",
    "# The number of possible actions that the agent may take in every step.\n",
    "num_actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment information : action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment information : observation (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/Breakout-v0/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm /home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/Breakout-v0/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl.checkpoint_base_dir = '/home/yanhua/Documents/jupyter/TensorFlow-Tutorials-master/checkpoints_tutorial16/'\n",
    "rl.checkpoint_base_dir = '/home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "rl.update_paths(env_name=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "Failed to restore checkpoint from: /home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/Breakout-v0\n",
      "Initializing variables instead.\n"
     ]
    }
   ],
   "source": [
    "#model = rl.NeuralNetwork(num_actions=num_actions,replay_memory= None)\n",
    "model = rl.NeuralNetwork(num_actions=num_actions,replay_memory= rl.ReplayMemory(size=200000,num_actions=num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'layer_conv1/weights:0' shape=(3, 3, 2, 16) dtype=float32_ref>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights_variable(layer_name='layer_conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd /home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/\n",
    "\n",
    "tensorboard --logdir=/home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/logs\n",
    "\n",
    "http://0.0.0.0:6006/\n",
    "\n",
    "convolution neural network 3 layer + 4 dense layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The formula for updating the Q-value is:\n",
    "\n",
    "$$\n",
    "    Q(s_{t},a_{t}) \\leftarrow \\underbrace{r_{t}}_{\\rm reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\rm estimate~of~future~rewards}\n",
    "$$\n",
    "\n",
    "### loss = q_values.l2_regression(target=q_values_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the initial neural network\n",
    "\n",
    "num_episodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episodes = 0\n",
    "num_episodes = 1\n",
    "end_episode = True\n",
    "count_states = model.get_count_states()\n",
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_greedy = rl.EpsilonGreedy(start_value=1.0,\n",
    "                                            end_value=0.1,\n",
    "                                            num_iterations=1e6,\n",
    "                                            num_actions=num_actions,\n",
    "                                            epsilon_testing=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "while count_episodes <= num_episodes:\n",
    "    if end_episode:\n",
    "        # Reset the game-environment and get the first image-frame.\n",
    "        img = env.reset()\n",
    "        # Create a new motion-tracer for processing images from the\n",
    "        # game-environment. Initialize with the first image-frame.\n",
    "        # This resets the motion-tracer so the trace starts again.\n",
    "        # This could also be done if end_life==True.\n",
    "        motion_tracer = rl.MotionTracer(img)\n",
    "        # Reset the reward for the entire episode to zero.\n",
    "        # This is only used for printing statistics.\n",
    "        reward_episode = 0.0\n",
    "\n",
    "        # Increase the counter for the number of episodes.\n",
    "        # This counter is stored inside the TensorFlow graph\n",
    "        # so it can be saved and restored with the checkpoint.\n",
    "        #count_episodes = model.increase_count_episodes()\n",
    "        count_episodes = count_episodes + 1\n",
    "        # Get the number of lives that the agent has left in this episode.\n",
    "        num_lives = env.unwrapped.ale.lives()\n",
    "\n",
    "    # Get the state of the game-environment from the motion-tracer.\n",
    "    # The state has two images: (1) The last image-frame from the game\n",
    "    # and (2) a motion-trace that shows movement trajectories.\n",
    "    state = motion_tracer.get_state()\n",
    "    # Use the Neural Network to estimate the Q-values for the state.\n",
    "    # Note that the function assumes an array of states and returns\n",
    "    # a 2-dim array of Q-values, but we just have a single state here.\n",
    "    q_values = model.get_q_values(states=[state])[0]\n",
    "    \n",
    "    action, epsilon = epsilon_greedy.get_action(q_values=q_values,\n",
    "                                                             iteration=count_states,\n",
    "                                                             training=training)\n",
    "    # Take a step in the game-environment using the given action.\n",
    "    # Note that in OpenAI Gym, the step-function actually repeats the\n",
    "    # action between 2 and 4 time-steps for Atari games, with the number\n",
    "    # chosen at random.\n",
    "    img, reward, end_episode, info = env.step(action)\n",
    "    # Process the image from the game-environment in the motion-tracer.\n",
    "    # This will first be used in the next iteration of the loop.\n",
    "    motion_tracer.process(image=img)\n",
    "    # Add the reward for the step to the reward for the entire episode.\n",
    "    reward_episode += reward\n",
    "\n",
    "    \n",
    "    # Render the game-environment to screen.\n",
    "    env.render()\n",
    "\n",
    "    # Insert a small pause to slow down the game,\n",
    "    # making it easier to follow for human eyes.\n",
    "    time.sleep(0.005)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_fraction = rl.LinearControlSignal(start_value=0.1,end_value=1.0,num_iterations=5e6)\n",
    "\n",
    "# The learning-rate for the optimizer decreases linearly.\n",
    "learning_rate_control = rl.LinearControlSignal(start_value=1e-3,end_value=1e-5,num_iterations=5e6)\n",
    "\n",
    "# The loss-limit is used to abort the optimization whenever the\n",
    "# mean batch-loss falls below this limit.\n",
    "loss_limit_control = rl.LinearControlSignal(start_value=0.1,end_value=0.015,num_iterations=5e6)\n",
    "\n",
    "# The maximum number of epochs to perform during optimization.\n",
    "# This is increased from 5 to 10 epochs, because it was found for\n",
    "max_epochs_control = rl.LinearControlSignal(start_value=5.0,end_value=10.0,num_iterations=5e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episodes = 0\n",
    "num_episodes = 100\n",
    "end_episode = True\n",
    "count_states = model.get_count_states()\n",
    "training=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trained neural network model about 80 hours\n",
    "\n",
    "$$\n",
    "    Q(state_{t},NOOP) \\leftarrow \\underbrace{r_{t}}_{\\rm reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{\\max_{a}Q(state_{t+1}, a)}_{\\rm estimate~of~future~rewards} = 1.0 + 0.97 \\cdot 1.830 \\simeq 2.775\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replay_memory.num_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1:\t Epsilon: 1.00\t Reward: 0.0\n",
      "   2:\t Epsilon: 1.00\t Reward: 0.0\n",
      "   3:\t Epsilon: 1.00\t Reward: 2.0\n",
      "   4:\t Epsilon: 1.00\t Reward: 1.0\n",
      "   5:\t Epsilon: 1.00\t Reward: 0.0\n",
      "   6:\t Epsilon: 1.00\t Reward: 2.0\n",
      "   7:\t Epsilon: 1.00\t Reward: 3.0\n",
      "   8:\t Epsilon: 1.00\t Reward: 2.0\n",
      "   9:\t Epsilon: 1.00\t Reward: 1.0\n",
      "  10:\t Epsilon: 1.00\t Reward: 1.0\n",
      "  11:\t Epsilon: 1.00\t Reward: 8.0\n",
      "  12:\t Epsilon: 1.00\t Reward: 2.0\n",
      "  13:\t Epsilon: 1.00\t Reward: 1.0\n",
      "  14:\t Epsilon: 1.00\t Reward: 2.0\n",
      "  15:\t Epsilon: 1.00\t Reward: 4.0\n",
      "  16:\t Epsilon: 1.00\t Reward: 0.0\n",
      "  17:\t Epsilon: 1.00\t Reward: 0.0\n",
      "  18:\t Epsilon: 1.00\t Reward: 1.0\n",
      "  19:\t Epsilon: 1.00\t Reward: 2.0\n",
      "  20:\t Epsilon: 1.00\t Reward: 2.0\n",
      "  21:\t Epsilon: 1.00\t Reward: 2.0\n",
      "  22:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  23:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  24:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  25:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  26:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  27:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  28:\t Epsilon: 0.99\t Reward: 4.0\n",
      "  29:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  30:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  31:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  32:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  33:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  34:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  35:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  36:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  37:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  38:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  39:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  40:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  41:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  42:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  43:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  44:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  45:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  46:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  47:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  48:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  49:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  50:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  51:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  52:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  53:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  54:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  55:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  56:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  57:\t Epsilon: 0.99\t Reward: 2.0\n",
      "  58:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  59:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  60:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  61:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  62:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  63:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  64:\t Epsilon: 0.99\t Reward: 3.0\n",
      "  65:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  66:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  67:\t Epsilon: 0.99\t Reward: 0.0\n",
      "  68:\t Epsilon: 0.99\t Reward: 1.0\n",
      "  69:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  70:\t Epsilon: 0.98\t Reward: 3.0\n",
      "  71:\t Epsilon: 0.98\t Reward: 3.0\n",
      "  72:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  73:\t Epsilon: 0.98\t Reward: 3.0\n",
      "  74:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  75:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  76:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  77:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  78:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  79:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  80:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  81:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  82:\t Epsilon: 0.98\t Reward: 3.0\n",
      "  83:\t Epsilon: 0.98\t Reward: 3.0\n",
      "  84:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  85:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  86:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  87:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  88:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  89:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  90:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  91:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  92:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  93:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  94:\t Epsilon: 0.98\t Reward: 0.0\n",
      "  95:\t Epsilon: 0.98\t Reward: 2.0\n",
      "  96:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  97:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  98:\t Epsilon: 0.98\t Reward: 1.0\n",
      "  99:\t Epsilon: 0.98\t Reward: 1.0\n",
      " 100:\t Epsilon: 0.98\t Reward: 1.0\n",
      "Replay-memory statistics:\n",
      "\tQ-values Before, Min: -0.01, Mean:  0.00, Max:  0.02\n",
      "\tQ-values After,  Min: -0.01, Mean:  0.01, Max:  1.30\n",
      "\tQ-values Diff.,  Min: -0.01, Mean:  0.00, Max:  1.30\n",
      "\tNumber of large errors > 0.1: 7046 / 24489 (28.8%)\n",
      "\tend_life: 2.0%, end_episode: 0.4%, reward non-zero: 0.5%\n",
      "Optimizing Neural Network to better estimate Q-values ...\n",
      "\tLearning-rate: 1.0e-03\n",
      "\tLoss-limit: 0.100\n",
      "\tMax epochs: 5.0\n",
      "\tIteration: 801 (4.19 epoch), Batch loss: 0.0936, Mean loss: 0.0996\n"
     ]
    }
   ],
   "source": [
    "while count_episodes <= num_episodes:\n",
    "    if end_episode:\n",
    "        # Reset the game-environment and get the first image-frame.\n",
    "        img = env.reset()\n",
    "        # Create a new motion-tracer for processing images from the\n",
    "        # game-environment. Initialize with the first image-frame.\n",
    "        # This resets the motion-tracer so the trace starts again.\n",
    "        # This could also be done if end_life==True.\n",
    "        motion_tracer = rl.MotionTracer(img)\n",
    "        # Reset the reward for the entire episode to zero.\n",
    "        # This is only used for printing statistics.\n",
    "        reward_episode = 0.0\n",
    "\n",
    "        # Increase the counter for the number of episodes.\n",
    "        # This counter is stored inside the TensorFlow graph\n",
    "        # so it can be saved and restored with the checkpoint.\n",
    "        count_episodes = count_episodes + 1\n",
    "        # Get the number of lives that the agent has left in this episode.\n",
    "        num_lives = env.unwrapped.ale.lives()\n",
    "\n",
    "    # Get the state of the game-environment from the motion-tracer.\n",
    "    # The state has two images: (1) The last image-frame from the game\n",
    "    # and (2) a motion-trace that shows movement trajectories.\n",
    "    state = motion_tracer.get_state()\n",
    "    # Use the Neural Network to estimate the Q-values for the state.\n",
    "    # Note that the function assumes an array of states and returns\n",
    "    # a 2-dim array of Q-values, but we just have a single state here.\n",
    "\n",
    "    q_values = model.get_q_values(states=[state])[0]\n",
    "    \n",
    "    action, epsilon = epsilon_greedy.get_action(q_values=q_values,\n",
    "                                                             iteration=count_states,\n",
    "                                                             training=training)\n",
    "    # Take a step in the game-environment using the given action.\n",
    "    # Note that in OpenAI Gym, the step-function actually repeats the\n",
    "    # action between 2 and 4 time-steps for Atari games, with the number\n",
    "    # chosen at random.\n",
    "    img, reward, end_episode, info = env.step(action)\n",
    "    # Process the image from the game-environment in the motion-tracer.\n",
    "    # This will first be used in the next iteration of the loop.\n",
    "    motion_tracer.process(image=img)\n",
    "    # Add the reward for the step to the reward for the entire episode.\n",
    "    reward_episode += reward\n",
    "    \n",
    "    # Determine if a life was lost in this step.\n",
    "    num_lives_new = env.unwrapped.ale.lives()\n",
    "    end_life = (num_lives_new < num_lives)\n",
    "    num_lives = num_lives_new\n",
    "\n",
    "    count_states = model.increase_count_states()\n",
    "\n",
    "    # Add the state of the game-environment to the replay-memory.\n",
    "    model.replay_memory.add(state=state,\n",
    "                                       q_values=q_values,\n",
    "                                       action=action,\n",
    "                                       reward=reward,\n",
    "                                       end_life=end_life,\n",
    "                                       end_episode=end_episode)\n",
    "\n",
    "    # How much of the replay-memory should be used.\n",
    "    use_fraction = replay_fraction.get_value(iteration=count_states)\n",
    "\n",
    "        \n",
    "    if end_episode:\n",
    "        msg = \"{0:4}:\\t Epsilon: {1:4.2f}\\t Reward: {2:.1f}\"\n",
    "        print(msg.format(count_episodes, epsilon,\n",
    "                                 reward_episode))\n",
    "    \n",
    "    # When the replay-memory is sufficiently full.\n",
    "#    if replay_memory.is_full() \\\n",
    "#                    or replay_memory.used_fraction() > use_fraction or count_episodes == num_episodes:\n",
    "    if model.replay_memory.is_full() \\\n",
    "                    or (count_episodes == num_episodes and end_episode):\n",
    "\n",
    "                    # Update all Q-values in the replay-memory through a backwards-sweep.\n",
    "        model.replay_memory.update_all_q_values()\n",
    "\n",
    "        # Get the control parameters for optimization of the Neural Network.\n",
    "        # These are changed linearly depending on the state-counter.\n",
    "        learning_rate = learning_rate_control.get_value(iteration=count_states)\n",
    "        loss_limit = loss_limit_control.get_value(iteration=count_states)\n",
    "        max_epochs = max_epochs_control.get_value(iteration=count_states)\n",
    "\n",
    "        # Perform an optimization run on the Neural Network so as to\n",
    "        # improve the estimates for the Q-values.\n",
    "        # This will sample random batches from the replay-memory.\n",
    "        \n",
    "        model.optimize(learning_rate=learning_rate,\n",
    "                                        loss_limit=loss_limit,\n",
    "                                        max_epochs=max_epochs)\n",
    "        \n",
    "        # Save a checkpoint of the Neural Network so we can reload it.\n",
    "#        self.model.save_checkpoint(count_states)\n",
    "\n",
    "        # Reset the replay-memory. This throws away all the data we have\n",
    "        # just gathered, so we will have to fill the replay-memory again.\n",
    "        model.replay_memory.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store the memory (image, reward, etc...) for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 105, 80, 2), (200000, 4), (200000,), (200000,), (200000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replay_memory.states.shape,model.replay_memory.q_values.shape,model.replay_memory.actions.shape,model.replay_memory.end_episode.shape,model.replay_memory.end_life.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replay_memory.rewards[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replay_memory.rewards[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the trained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episodes = 0\n",
    "num_episodes = 3\n",
    "end_episode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1:\tQ-min: 0.007\tQ-max: 0.021\tLives: 5\tReward: 1.0\n",
      "   2:\tQ-min: 0.030\tQ-max: 0.040\tLives: 5\tReward: 0.0\n",
      "   3:\tQ-min: 0.005\tQ-max: 0.019\tLives: 5\tReward: 0.0\n"
     ]
    }
   ],
   "source": [
    "while count_episodes <= num_episodes:\n",
    "    if end_episode:\n",
    "        # Reset the game-environment and get the first image-frame.\n",
    "        img = env.reset()\n",
    "        # Create a new motion-tracer for processing images from the\n",
    "        # game-environment. Initialize with the first image-frame.\n",
    "        # This resets the motion-tracer so the trace starts again.\n",
    "        # This could also be done if end_life==True.\n",
    "        motion_tracer = rl.MotionTracer(img)\n",
    "        # Reset the reward for the entire episode to zero.\n",
    "        # This is only used for printing statistics.\n",
    "        reward_episode = 0.0\n",
    "\n",
    "        # Increase the counter for the number of episodes.\n",
    "        # This counter is stored inside the TensorFlow graph\n",
    "        # so it can be saved and restored with the checkpoint.\n",
    "        #count_episodes = model.increase_count_episodes()\n",
    "        count_episodes = count_episodes + 1\n",
    "        # Get the number of lives that the agent has left in this episode.\n",
    "        num_lives = env.unwrapped.ale.lives()\n",
    "\n",
    "    # Get the state of the game-environment from the motion-tracer.\n",
    "    # The state has two images: (1) The last image-frame from the game\n",
    "    # and (2) a motion-trace that shows movement trajectories.\n",
    "    state = motion_tracer.get_state()\n",
    "    # Use the Neural Network to estimate the Q-values for the state.\n",
    "    # Note that the function assumes an array of states and returns\n",
    "    # a 2-dim array of Q-values, but we just have a single state here.\n",
    "    q_values = model.get_q_values(states=[state])[0]\n",
    "    \n",
    "    action, epsilon = epsilon_greedy.get_action(q_values=q_values,\n",
    "                                                             iteration=count_states,\n",
    "                                                             training=training)\n",
    "    # Take a step in the game-environment using the given action.\n",
    "    # Note that in OpenAI Gym, the step-function actually repeats the\n",
    "    # action between 2 and 4 time-steps for Atari games, with the number\n",
    "    # chosen at random.\n",
    "    img, reward, end_episode, info = env.step(action)\n",
    "    # Process the image from the game-environment in the motion-tracer.\n",
    "    # This will first be used in the next iteration of the loop.\n",
    "    motion_tracer.process(image=img)\n",
    "    # Add the reward for the step to the reward for the entire episode.\n",
    "    reward_episode += reward\n",
    "\n",
    "    \n",
    "    # Render the game-environment to screen.\n",
    "    env.render()\n",
    "\n",
    "    # Insert a small pause to slow down the game,\n",
    "    # making it easier to follow for human eyes.\n",
    "    time.sleep(0.01)\n",
    "    # Print Q-values and reward to screen.\n",
    "    if(end_life or end_episode):\n",
    "        msg = \"{0:4}:\\tQ-min: {1:5.3f}\\tQ-max: {2:5.3f}\\tLives: {3}\\tReward: {4:.1f}\"\n",
    "        print(msg.format(count_episodes, np.min(q_values),np.max(q_values), num_lives, reward_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24490"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/Breakout-v0/checkpoint'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint.\n"
     ]
    }
   ],
   "source": [
    "model.save_checkpoint(count_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint.\n"
     ]
    }
   ],
   "source": [
    "model.saver.save(model.session,save_path='/home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/checkporint/breakout/',global_step=count_states)\n",
    "print(\"Saved checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                            checkpoint-24490.index\r\n",
      "checkpoint-24490.data-00000-of-00001  checkpoint-24490.meta\r\n"
     ]
    }
   ],
   "source": [
    "ls /home/yanhua/Documents/jupyter/yan/demo/Deep_Q_Network/checkpoints_tutorial/Breakout-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Pre-trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is trained about 80 hours (3 days) \n",
    "\n",
    "restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanhua/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from /home/yanhua/Documents/jupyter/TensorFlow-Tutorials-master/checkpoints_tutorial16/Breakout-v0/checkpoint-253779762\n",
      "Restored checkpoint from: /home/yanhua/Documents/jupyter/TensorFlow-Tutorials-master/checkpoints_tutorial16/Breakout-v0/checkpoint-253779762\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import prettytensor as pt\n",
    "import scipy.ndimage\n",
    "import time\n",
    "import sys\n",
    "import reinforcement_learning as rl\n",
    "from reinforcement_learning import ReplayMemory\n",
    "env = gym.make('Breakout-v0')\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "MEMORY_SIZE = 3000\n",
    "ACTION_SPACE = 11\n",
    "# The number of possible actions that the agent may take in every step.\n",
    "num_actions = env.action_space.n\n",
    "epsilon_greedy = rl.EpsilonGreedy(start_value=1.0,\n",
    "                                            end_value=0.1,\n",
    "                                            num_iterations=1e6,\n",
    "                                            num_actions=num_actions,\n",
    "                                            epsilon_testing=0.01)\n",
    "rl.checkpoint_base_dir = '/home/yanhua/Documents/jupyter/TensorFlow-Tutorials-master/checkpoints_tutorial16/'\n",
    "env_name = 'Breakout-v0'\n",
    "rl.update_paths(env_name=env_name)\n",
    "model = rl.NeuralNetwork(num_actions=num_actions,replay_memory= rl.ReplayMemory(size=200000,num_actions=num_actions))\n",
    "count_states = model.get_count_states()\n",
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_episodes = 0\n",
    "num_episodes = 2\n",
    "end_episode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1:\tQ-min: -0.335\tQ-max: -0.235\tLives: 5\tReward: 98.0\n",
      "   2:\tQ-min: -0.420\tQ-max: 0.043\tLives: 5\tReward: 59.0\n"
     ]
    }
   ],
   "source": [
    "while count_episodes <= num_episodes:\n",
    "    if end_episode:\n",
    "        # Reset the game-environment and get the first image-frame.\n",
    "        img = env.reset()\n",
    "        # Create a new motion-tracer for processing images from the\n",
    "        # game-environment. Initialize with the first image-frame.\n",
    "        # This resets the motion-tracer so the trace starts again.\n",
    "        # This could also be done if end_life==True.\n",
    "        motion_tracer = rl.MotionTracer(img)\n",
    "        # Reset the reward for the entire episode to zero.\n",
    "        # This is only used for printing statistics.\n",
    "        reward_episode = 0.0\n",
    "\n",
    "        # Increase the counter for the number of episodes.\n",
    "        # This counter is stored inside the TensorFlow graph\n",
    "        # so it can be saved and restored with the checkpoint.\n",
    "        #count_episodes = model.increase_count_episodes()\n",
    "        count_episodes = count_episodes + 1\n",
    "        # Get the number of lives that the agent has left in this episode.\n",
    "        num_lives = env.unwrapped.ale.lives()\n",
    "\n",
    "    # Get the state of the game-environment from the motion-tracer.\n",
    "    # The state has two images: (1) The last image-frame from the game\n",
    "    # and (2) a motion-trace that shows movement trajectories.\n",
    "    state = motion_tracer.get_state()\n",
    "    # Use the Neural Network to estimate the Q-values for the state.\n",
    "    # Note that the function assumes an array of states and returns\n",
    "    # a 2-dim array of Q-values, but we just have a single state here.\n",
    "    q_values = model.get_q_values(states=[state])[0]\n",
    "    \n",
    "    action, epsilon = epsilon_greedy.get_action(q_values=q_values,\n",
    "                                                             iteration=count_states,\n",
    "                                                             training=training)\n",
    "    # Take a step in the game-environment using the given action.\n",
    "    # Note that in OpenAI Gym, the step-function actually repeats the\n",
    "    # action between 2 and 4 time-steps for Atari games, with the number\n",
    "    # chosen at random.\n",
    "    img, reward, end_episode, info = env.step(action)\n",
    "    # Process the image from the game-environment in the motion-tracer.\n",
    "    # This will first be used in the next iteration of the loop.\n",
    "    motion_tracer.process(image=img)\n",
    "    # Add the reward for the step to the reward for the entire episode.\n",
    "    reward_episode += reward\n",
    "\n",
    "    \n",
    "    # Render the game-environment to screen.\n",
    "    env.render()\n",
    "\n",
    "    # Insert a small pause to slow down the game,\n",
    "    # making it easier to follow for human eyes.\n",
    "    time.sleep(0.005)\n",
    "    # Print Q-values and reward to screen.\n",
    "    if(end_episode):\n",
    "        msg = \"{0:4}:\\tQ-min: {1:5.3f}\\tQ-max: {2:5.3f}\\tLives: {3}\\tReward: {4:.1f}\"\n",
    "        print(msg.format(count_episodes, np.min(q_values),np.max(q_values), num_lives, reward_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
